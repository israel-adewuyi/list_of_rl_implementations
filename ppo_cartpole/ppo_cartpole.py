import time
import torch
import numpy as np
import torch.nn as nn
import gymnasium as gym

from utils import make_env
from dataclasses import dataclass
from typing import Literal, Tuple
from pathlib import Path

root_dir = Path.cwd()

@dataclass
class PPOArgs:
    # Basic / global
    seed: int = 1
    env_id: str = "CartPole-v1"
    mode: Literal["classic-control", "atari", "mujoco"] = "classic-control"

    # Wandb / logging
    use_wandb: bool = False
    video_log_freq: int | None = None
    wandb_project_name: str = "PPOCartPole"
    wandb_entity: str = None

    # Duration of different phases
    total_timesteps: int = 500_000
    num_envs: int = 4
    num_steps_per_rollout: int = 128
    num_minibatches: int = 4
    batches_per_learning_phase: int = 4

    # Optimization hyperparameters
    lr: float = 2.5e-4
    max_grad_norm: float = 0.5

    # RL hyperparameters
    gamma: float = 0.99

    # PPO-specific hyperparameters
    gae_lambda: float = 0.95
    clip_coef: float = 0.2
    ent_coef: float = 0.01
    vf_coef: float = 0.25

    # training device
    device: str = "cuda:0"

    def __post_init__(self):
        self.batch_size = self.num_steps_per_rollout * self.num_envs

        assert self.batch_size % self.num_minibatches == 0, "batch_size must be divisible by num_minibatches"
        self.minibatch_size = self.batch_size // self.num_minibatches
        self.total_phases = self.total_timesteps // self.batch_size
        self.total_training_steps = self.total_phases * self.batches_per_learning_phase * self.num_minibatches

        self.video_save_path = root_dir / "videos"

def layer_init(layer: nn.Linear, std=np.sqrt(2), bias_const=0.0):
    torch.nn.init.orthogonal_(layer.weight, std)
    torch.nn.init.constant_(layer.bias, bias_const)
    return layer

def get_actor_critic(
    envs: gym.vector.SyncVectorEnv,
    device: str, 
    mode: Literal["classic-control", "atari", "mujoco"] = "classic-control",
) -> Tuple[nn.Module, nn.Module]:

    assert mode in ["classic-control", "atari", "mujoco"], mode

    obs_shape = envs.single_observation_space.shape
    num_obs = np.array(obs_shape).prod()
    num_actions = (
        envs.single_action_space.n
        if isinstance(envs.single_action_space, gym.spaces.Discrete)
        else np.array(envs.single_action_space.shape).prod()
    )


    # debugging these guys. Remove if correct
    print(f"Shape of a single observation is {obs_shape}")

    if mode == "classic-control":
        actor, critic = get_actor_and_critic_classic(num_obs, num_actions)
    if mode == "atari":
        actor, critic = get_actor_and_critic_atari(obs_shape, num_actions) 
    if mode == "mujoco":
        actor, critic = get_actor_and_critic_mujoco(num_obs, num_actions)

    return actor.to(device), critic.to(device)

def get_actor_and_critic_classic(num_obs: int, num_actions: int) -> Tuple[nn.Module, nn.Module]:
    actor = nn.Sequential(
        layer_init(nn.Linear(num_obs, 64)),
        nn.Tanh(),
        layer_init(nn.Linear(64, 64)),
        nn.Tanh(), 
        layer_init(nn.Linear(64, num_actions), std=0.01)
    )
    critic = nn.Sequential(
        layer_init(nn.Linear(num_obs, 64)),
        nn.Tanh(),
        layer_init(nn.Linear(64, 64)),
        nn.Tanh(), 
        layer_init(nn.Linear(64, 1), std=0.1)
    )

    return actor, critic

def get_minibatch_indices(rng: Generator, batch_size: int, minibatch_size: int) -> list[np.ndarray]:
    """
    Return a list of length `num_minibatches`, where each element is an array of `minibatch_size` 
    and the union of all the arrays is the set of indices [0, 1, ..., batch_size - 1] where 
    `batch_size = num_steps_per_rollout * num_envs`.
    """
    assert batch_size % minibatch_size == 0
    num_minibatches = batch_size // minibatch_size
    indices = np.arange(batch_size)
    indices = rng.shuffle(indices)
    indices = indices.reshape((num_minibatches, minibatch_size))
    return list(indices)


class PPOTrainer:
    def __init__(self, args: PPOArgs):
        self.args = args
        self.run_name = f"{args.env_id}__{args.wandb_project_name}__seed{args.seed}__{time.strftime('%Y%m%d-%H%M%S')}"
        self.envs = gym.vector.SyncVectorEnv(
            [make_env(idx=idx, run_name=self.run_name, **args.__dict__) for idx in range(args.num_envs)]
        )

        self.actor, self.critic = get_actor_critic(self.envs, self.args.device, self.args.mode)


args = PPOArgs(num_minibatches=2)  # changing this also changes minibatch_size and total_training_steps
trainer = PPOTrainer(args)














